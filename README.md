## Lifelong-Learning

## Literuture Review
#### [Continual Lifelong Learning with Neural Networks:A Review](https://arxiv.org/pdf/1802.07569.pdf)


## Regularization Approaches
- <a name="EWC"></a> Overcoming catastrophic forgetting in neural networks (EWC) (**PNAS2017**) [[paper](https://arxiv.org/abs/1612.00796)] [[code](https://github.com/ariseff/overcoming-catastrophic)] [[code](https://github.com/stokesj/EWC)]
- <a name="SI"></a> Continual Learning Through Synaptic Intelligence (**ICML2017**) [[paper](http://proceedings.mlr.press/v70/zenke17a.html)] [[code](https://github.com/ganguli-lab/pathint)]


## Memorey-based Approaches
- <a name="todo"></a> iCaRL: Incremental Classifier and Representation Learning (**CVPR2017**) [[paper](https://arxiv.org/abs/1611.07725)] [[code](https://github.com/srebuffi/iCaRL)]
- <a name="GEM"></a> Gradient Episodic Memory for Continual Learning (**NIPS2017**) [[paper](https://arxiv.org/abs/1706.08840)] [[code](https://github.com/facebookresearch/GradientEpisodicMemory)]
 - <a name="todo"></a> Memory Aware Synapses: Learning what (not) to forget (**ECCV2018**) [[paper](https://arxiv.org/abs/1711.09601)] [[code](https://github.com/rahafaljundi/MAS-Memory-Aware-Synapses)]
- <a name="A-GEM"></a> Efficient Lifelong Learning with A-GEM (**ICLR2019**) [[paper](https://openreview.net/forum?id=Hkf2_sC5FX)] [[code](https://github.com/facebookresearch/agem)]

## Architecture Approaches
- <a name="PNN"></a> Progressive Neural Networks [[paper](https://arxiv.org/abs/1606.04671)]
- <a name="DEN"></a> Lifelong Learning with Dynamically Expandable Networks (**ICLR2018**) [[paper](https://arxiv.org/abs/1708.01547)]
- <a name="todo"></a> Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting (**ICML2019**) [[paper](https://arxiv.org/abs/1904.00310)]


## Incremental/Lifelong Learning Metrics
- <a name="todo"></a> Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence (**ECCV2018**)[[paper](http://arxiv-export-lb.library.cornell.edu/abs/1801.10112)] 
- <a name="todo"></a> Learning a Unified Classifier Incrementally via Rebalancing (**CVPR2019**) [[paper](http://openaccess.thecvf.com/content_CVPR_2019/papers/Hou_Learning_a_Unified_Classifier_Incrementally_via_Rebalancing_CVPR_2019_paper.pdf)] 

## Network Pruning
- <a name="Pruning"></a> TensorFlow Model Optimization Toolkit â€” Pruning API
 [[Source](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-pruning-api-42cac9157a6a)][[code](https://github.com/tensorflow/model-optimization/tree/master/tensorflow_model_optimization/python/examples/sparsity/keras)]

- <a name="LotteryTicket"></a> The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (**ICLR2019**)[[paper](https://arxiv.org/abs/1803.03635)] 

- <a name="LotteryTicket"></a> Sparse Transfer Learning via Winning LotteryTickets (**CoRR2019**)[[paper]( https://arxiv.org/abs/1905.07785)] 

